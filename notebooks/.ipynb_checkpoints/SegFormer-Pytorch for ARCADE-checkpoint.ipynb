{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f6718c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\prati\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from glob import glob\n",
    "from datasets import Dataset, load_dataset, load_metric, DatasetDict\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision.transforms.v2 import ToTensor\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision.transforms import v2 as v2\n",
    "from transformers import SegformerConfig, SegformerImageProcessor, SegformerForSemanticSegmentation\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640e8c2",
   "metadata": {},
   "source": [
    "#### BREAK POINT ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "9877abce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.classifier.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.weight', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.batch_norm.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.running_mean', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model loading and preprocessing\n",
    "image_processor = SegformerImageProcessor.from_pretrained(\"nvidia/mit-b0\")\n",
    "image_processor.do_reduce_labels = True\n",
    "\n",
    "id2label = {0 : \"Background\", 1 : \"Plaque\"}\n",
    "label2id = {\"Background\" : 0, \"Plaque\" : 1}\n",
    "\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b0\",\n",
    "                                                         num_labels=2,\n",
    "                                                         id2label=id2label,\n",
    "                                                         label2id=label2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "9ec73556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# img = Image.open(\"../data/processed/train/images/1.png\")\n",
    "# msk = Image.open(\"../data/processed/train/masks/1.png\")\n",
    "\n",
    "# img2 = Image.open(\"../data/processed/train/images/2.png\")\n",
    "# msk2 = Image.open(\"../data/processed/train/masks/2.png\")\n",
    "\n",
    "img = cv2.imread(\"../data/processed/train/images/1.png\")\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "msk = cv2.imread(\"../data/processed/train/masks/1.png\")\n",
    "msk = cv2.cvtColor(msk, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "img2 = cv2.imread(\"../data/processed/train/images/2.png\")\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "msk2 = cv2.imread(\"../data/processed/train/masks/2.png\")\n",
    "msk2 = cv2.cvtColor(msk2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "msk = msk // 255\n",
    "msk2 = msk2 // 255\n",
    "\n",
    "img_com = [img, img2]\n",
    "msk_com = [msk, msk2]\n",
    "\n",
    "# img_com = image_processor(img_com, msk_com, return_tensors = \"pt\")\n",
    "\n",
    "img_com = np.array(img_com)\n",
    "img_com = img_com.transpose(0, 3, 1, 2)\n",
    "img_com = torch.tensor(img_com, dtype = torch.float32)\n",
    "msk_com = torch.tensor(np.array(msk_com), dtype = torch.long)\n",
    "\n",
    "len(img_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "9c7df4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6960, grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(pixel_values = img_com, labels = msk_com)\n",
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "507d075a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6960, grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "710d42e8",
   "metadata": {},
   "source": [
    "## Dataset Object Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f3f4edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import TrainingArguments\n",
    "from transformers import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "e3d5bc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prati\\anaconda3\\Lib\\site-packages\\transformers\\models\\segformer\\image_processing_segformer.py:100: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.classifier.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.weight', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.batch_norm.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.running_mean', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model loading and preprocessing\n",
    "image_processor = SegformerImageProcessor.from_pretrained(\"nvidia/mit-b0\")\n",
    "image_processor.do_reduce_labels = True\n",
    "\n",
    "id2label = {0 : \"Background\", 1 : \"Plaque\"}\n",
    "label2id = {\"Background\" : 0, \"Plaque\" : 1}\n",
    "\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b0\",\n",
    "                                                         num_labels=2,\n",
    "                                                         id2label=id2label,\n",
    "                                                         label2id=label2id)\n",
    "model.config.semantic_loss_ignore_index = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "4a04faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, root_path, file_names, dataset_type):\n",
    "        self.root_path = root_path\n",
    "        self.file_names = file_names\n",
    "        self.dataset_type = dataset_type\n",
    "        self.image_processor = SegformerImageProcessor.from_pretrained(\"nvidia/mit-b0\")\n",
    "         \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img_index = self.file_names[index]\n",
    "        \n",
    "        img_path = os.path.join(self.root_path, self.dataset_type, \"images\", img_index)\n",
    "        msk_path = os.path.join(self.root_path, self.dataset_type, \"masks\",  img_index)\n",
    "        \n",
    "        \n",
    "        # read the images\n",
    "#         img = Image.open(img_path)\n",
    "#         msk = Image.open(msk_path)\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        msk = cv2.imread(msk_path)\n",
    "        msk = cv2.cvtColor(msk, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # normalize the mask to have category of 0 and 1\n",
    "        msk = msk // 255\n",
    "        \n",
    "        model_inputs = self.image_processor(images = img,\n",
    "                                            segmentation_maps=msk,\n",
    "                                            return_tensors = \"pt\")\n",
    "        \n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "83cdbe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "# Train dataset\n",
    "train_img_files = glob(\"../data/processed/train/images/*\")\n",
    "train_msk_files = glob(\"../data/processed/train/masks/*\")\n",
    "\n",
    "# Test dataset\n",
    "test_img_files = glob(\"../data/processed/test/images/*\")\n",
    "test_msk_files = glob(\"../data/processed/test/masks/*\")\n",
    "\n",
    "# Validation dataset\n",
    "val_img_files = glob(\"../data/processed/val/images/*\")\n",
    "val_msk_files = glob(\"../data/processed/val/masks/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "a2382ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = []\n",
    "test_files = []\n",
    "validation_files = []\n",
    "\n",
    "for file in train_img_files:\n",
    "    train_files.append(file.split(sep = \"\\\\\")[1])\n",
    "\n",
    "for file in test_img_files:\n",
    "    test_files.append(file.split(sep = \"\\\\\")[1])\n",
    "\n",
    "for file in val_img_files:\n",
    "    validation_files.append(file.split(sep = \"\\\\\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0eefb97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_files = [f\"{ind}.png\" for ind in range(1, 998)]\n",
    "# test_files = [f\"{ind}.png\" for ind in range(1, 301)]\n",
    "# validation_files = [f\"{ind}.png\" for ind in range(1, 201)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "665a1e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function to process a batch of images.\n",
    "    \"\"\"\n",
    "    pixel_values = [torch.squeeze(example['pixel_values'], dim = 0) \n",
    "                    for example in batch]\n",
    "    labels = [torch.squeeze(example['labels'], dim = 0) for example in batch]\n",
    "    \n",
    "    # Stack images into a single tensor\n",
    "    pixel_values = torch.stack(pixel_values, dim = 0)\n",
    "    # Convert labels to tensor\n",
    "    labels = torch.stack(labels, dim = 0)\n",
    "    \n",
    "    \n",
    "    return {'pixel_values': pixel_values, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "40263053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Object\n",
    "train_dataset = SegmentationDataset(root_path = \"../data/processed/\",\n",
    "                                    file_names = train_files,\n",
    "                                    dataset_type = \"train\")\n",
    "\n",
    "test_dataset = SegmentationDataset(root_path = \"../data/processed/\",\n",
    "                                   file_names = test_files,\n",
    "                                   dataset_type = \"test\")\n",
    "\n",
    "validation_dataset = SegmentationDataset(root_path = \"../data/processed/\",\n",
    "                                         file_names = validation_files,\n",
    "                                         dataset_type = \"val\")\n",
    "\n",
    "# Dataloader object\n",
    "train_dataloader = DataLoader(dataset = train_dataset,\n",
    "                              batch_size = 8,\n",
    "                              shuffle = True,\n",
    "                              collate_fn = collate_fn)\n",
    "test_dataloader = DataLoader(dataset = test_dataset,\n",
    "                              batch_size = 8,\n",
    "                              shuffle = True,\n",
    "                              collate_fn = collate_fn)\n",
    "val_dataloader = DataLoader(dataset = validation_dataset,\n",
    "                            batch_size = 8,\n",
    "                            shuffle = True,\n",
    "                            collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "c467df23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 512, 512]) torch.Size([8, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "# Testing the dataloader object\n",
    "batch = next(iter(val_dataloader))\n",
    "\n",
    "print(batch[\"pixel_values\"].shape, batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "e4307967",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 4\n",
    "TOTAL_STEPS = len(train_dataloader) * NUM_EPOCHS\n",
    "LEARNING_RATE = 0.001\n",
    "GRADIENT_ACCUMULATION = 2\n",
    "\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir = \"../experiments/experiment1/\",\n",
    "    overwrite_output_dir = True,\n",
    "    do_train = True,\n",
    "    do_eval = True,\n",
    "    do_predict = True,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    save_total_limit = 2,\n",
    "    prediction_loss_only = False,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "    gradient_accumulation_steps = GRADIENT_ACCUMULATION,\n",
    "    eval_accumulation_steps = GRADIENT_ACCUMULATION,\n",
    "    save_strategy = \"steps\", \n",
    "    eval_steps = 10,\n",
    "    num_train_epochs = NUM_EPOCHS,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    logging_strategy = \"steps\",\n",
    "    logging_steps = 10,\n",
    "    load_best_model_at_end = True,\n",
    "    learning_rate = LEARNING_RATE, fp16 = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "99a16819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import evaluate\n",
    "\n",
    "############################# THIS CODE IS TAKEN FROM THE HUGGINGFACE TRANSFORMERS #######################################\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "      with torch.no_grad():\n",
    "        logits, labels = eval_pred\n",
    "        logits_tensor = torch.from_numpy(logits)\n",
    "        # scale the logits to the size of the label\n",
    "        logits_tensor = nn.functional.interpolate(\n",
    "            logits_tensor,\n",
    "            size=labels.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        ).argmax(dim=1)\n",
    "\n",
    "        pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "        # currently using _compute instead of compute\n",
    "        # see this issue for more info: https://github.com/huggingface/evaluate/pull/328#issuecomment-1286866576\n",
    "        metrics = metric._compute(\n",
    "                predictions=pred_labels,\n",
    "                references=labels,\n",
    "                num_labels=len(id2label),\n",
    "                ignore_index=0,\n",
    "                reduce_labels=image_processor.do_reduce_labels,\n",
    "            )\n",
    "\n",
    "        # add per category metrics as individual key-value pairs\n",
    "        per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n",
    "        per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n",
    "\n",
    "        metrics.update({f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)})\n",
    "        metrics.update({f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)})\n",
    "\n",
    "        return metrics\n",
    "\n",
    "##############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "3012175c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "model = model.to(\"cuda\")\n",
    "trainer = Trainer(model = model,\n",
    "                  args = training_arguments,\n",
    "                  data_collator = collate_fn, \n",
    "                  train_dataset = train_dataset,\n",
    "                  eval_dataset= validation_dataset,\n",
    "                  compute_metrics = compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "98681c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  12/1250 01:20 < 2:46:12, 0.12 it/s, Epoch 0.09/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mean Iou</th>\n",
       "      <th>Mean Accuracy</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "      <th>Accuracy Background</th>\n",
       "      <th>Accuracy Plaque</th>\n",
       "      <th>Iou Background</th>\n",
       "      <th>Iou Plaque</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.435100</td>\n",
       "      <td>0.183646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prati\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--mean_iou\\08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9\\mean_iou.py:258: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  all_acc = total_area_intersect.sum() / total_area_label.sum()\n",
      "C:\\Users\\prati\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--mean_iou\\08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9\\mean_iou.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  iou = total_area_intersect / total_area_union\n",
      "C:\\Users\\prati\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--mean_iou\\08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9\\mean_iou.py:263: RuntimeWarning: Mean of empty slice\n",
      "  metrics[\"mean_accuracy\"] = np.nanmean(acc)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-324-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1554\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1555\u001b[1;33m             return inner_training_loop(\n\u001b[0m\u001b[0;32m   1556\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1557\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1840\u001b[0m                     \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m                     \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1842\u001b[1;33m                     \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1843\u001b[0m                 ):\n\u001b[0;32m   1844\u001b[0m                     \u001b[1;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "b81e55ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.017016639932990074,\n",
       " 'eval_mean_iou': 1.0,\n",
       " 'eval_mean_accuracy': 1.0,\n",
       " 'eval_overall_accuracy': 1.0,\n",
       " 'eval_accuracy_Background': nan,\n",
       " 'eval_accuracy_Plaque': 1.0,\n",
       " 'eval_iou_Background': nan,\n",
       " 'eval_iou_Plaque': 1.0}"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset = validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "0ed32fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cpu\")\n",
    "out = model(**img_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "47bb2827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msk = torch.tensor(msk, dtype = torch.long)\n",
    "msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "0ac72a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7141)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
